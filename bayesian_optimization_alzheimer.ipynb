{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAZFrfAniZKk"
      },
      "outputs": [],
      "source": [
        "pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZArIMij7gPXu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD2NtEJViqVl"
      },
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "import os \n",
        "from os.path import join\n",
        "import tqdm\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "from skimage.color import grey2rgb\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, AveragePooling2D, MaxPooling2D, Conv2D, SeparableConv2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, \\\n",
        "    confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "import PIL\n",
        "import random\n",
        "import cv2 as cv\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvjtUkQpLakV"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "    \n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2XNN5iSIo47"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [224, 224]\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z7goiSHIp4B"
      },
      "outputs": [],
      "source": [
        "data_folder = \"/content/drive/MyDrive/MS-Upgrad/alzheimers-1\"\n",
        "\n",
        "## images NonDemented\n",
        "df_NonDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"NonDemented\", \"*\"))),\n",
        "    \"y\": 0,\n",
        "    \"class\": \"NonDementia\"\n",
        "})\n",
        "shuffled_non = df_NonDemented_imgs.sample(frac=1)\n",
        "testsize_non = int(0.2 * len(shuffled_non))\n",
        "# Test\n",
        "non_test = shuffled_non[:testsize_non]\n",
        "# Train and validation\n",
        "non_trainval = shuffled_non[testsize_non:]\n",
        "trainsize_non = int(0.8 * len(non_trainval))\n",
        "# Train\n",
        "shuffled_non_train = non_trainval[:trainsize_non]\n",
        "#Validation\n",
        "shuffled_non_val = non_trainval[trainsize_non:]\n",
        "\n",
        "\n",
        "## images VeryMildDemented\n",
        "df_VeryMildDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"VeryMildDemented\", \"*\"))),\n",
        "    \"y\": 1,\n",
        "    \"class\": \"VeryMildDementia\"\n",
        "})\n",
        "shuffled_verymild = df_VeryMildDemented_imgs.sample(frac=1)\n",
        "testsize_verymild = int(0.2 * len(shuffled_verymild))\n",
        "\n",
        "# Test\n",
        "verymild_test = shuffled_verymild[:testsize_verymild]\n",
        "\n",
        "# Train and validation\n",
        "verymild_trainval = shuffled_verymild[testsize_verymild:]\n",
        "trainsize_verymild = int(0.8 * len(verymild_trainval))\n",
        "\n",
        "# Train\n",
        "shuffled_verymild_train = verymild_trainval[:trainsize_verymild]\n",
        "\n",
        "#Validation\n",
        "shuffled_verymild_val = verymild_trainval[trainsize_verymild:]\n",
        "\n",
        "\n",
        "## images MildDemented\n",
        "df_MildDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"MildDemented\", \"*\"))),\n",
        "    \"y\": 2,\n",
        "    \"class\": \"MildDementia\"\n",
        "})\n",
        "shuffled_mild = df_MildDemented_imgs.sample(frac=1)\n",
        "testsize_mild = int(0.2 * len(shuffled_mild))\n",
        "\n",
        "# Test\n",
        "mild_test = shuffled_mild[:testsize_mild]\n",
        "\n",
        "# Train and validation\n",
        "mild_trainval = shuffled_mild[testsize_mild:]\n",
        "trainsize_mild = int(0.8 * len(mild_trainval))\n",
        "\n",
        "# Train\n",
        "shuffled_mild_train = mild_trainval[:trainsize_mild]\n",
        "\n",
        "#Validation\n",
        "shuffled_mild_val = mild_trainval[trainsize_mild:]\n",
        "\n",
        "## images ModerateDemented\n",
        "df_ModerateDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"ModerateDemented\", \"*\"))),\n",
        "    \"y\": 3,\n",
        "    \"class\": \"ModerateDementia\"\n",
        "})\n",
        "shuffled_moderate = df_ModerateDemented_imgs.sample(frac=1)\n",
        "testsize_moderate = int(0.2 * len(shuffled_moderate))\n",
        "\n",
        "# Test\n",
        "moderate_test = shuffled_moderate[:testsize_moderate]\n",
        "\n",
        "# Train and validation\n",
        "moderate_trainval = shuffled_moderate[testsize_moderate:]\n",
        "trainsize_moderate = int(0.8 * len(moderate_trainval))\n",
        "\n",
        "# Train\n",
        "shuffled_moderate_train = moderate_trainval[:trainsize_moderate]\n",
        "\n",
        "#Validation\n",
        "shuffled_moderate_val = moderate_trainval[trainsize_moderate:]\n",
        "\n",
        "\n",
        "## Number of images\n",
        "print(\"TOTAL:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(shuffled_non))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(shuffled_verymild))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(shuffled_mild))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(shuffled_moderate))\n",
        "print(\"------------\")\n",
        "print(\"\\nTest:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(non_test))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(verymild_test))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(mild_test))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(moderate_test))\n",
        "print(\"------------\")\n",
        "print(\"\\nTraining:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(shuffled_non_train))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(shuffled_verymild_train))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(shuffled_mild_train))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(shuffled_moderate_train))\n",
        "print(\"------------\")\n",
        "print(\"\\nValidation:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(shuffled_non_val))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(shuffled_verymild_val))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(shuffled_mild_val))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(shuffled_moderate_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCA7JHodll4Y"
      },
      "outputs": [],
      "source": [
        "# show class imbalance in the dataset\n",
        "\n",
        "heights = [len(shuffled_non), len(shuffled_verymild), len(shuffled_mild), len(shuffled_moderate)]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "height = heights\n",
        "bars = ('Non', 'Very Mild', 'Mild', 'Moderate')\n",
        "y_pos = np.arange(len(bars))\n",
        "plt.bar(y_pos, height)\n",
        "plt.xticks(y_pos, bars)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw2INqOSlaQ-"
      },
      "outputs": [],
      "source": [
        "def load_img(fname):\n",
        "    img = cv.imread(fname)\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "    resize = (IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
        "    img = cv.resize(img, resize)\n",
        "    return img\n",
        "\n",
        "def one_hot_encoding(class_number):\n",
        "    ha = [0,0,0,0]\n",
        "    ha[class_number] = 1\n",
        "    return ha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnhPgFzdle78"
      },
      "outputs": [],
      "source": [
        "## TRAIN SET\n",
        "# concatenate\n",
        "train_concat = pd.concat([shuffled_mild_train, shuffled_moderate_train, shuffled_non_train, shuffled_verymild_train])\n",
        "# shuffle\n",
        "train_concat = train_concat.sample(frac=1)\n",
        "# load image\n",
        "train_concat[\"X\"] = train_concat[\"X\"].apply(load_img)\n",
        "\n",
        "y_train_lab = train_concat[\"y\"]\n",
        "y_train_lab = np.array(y_train_lab) # convert into a numpy array\n",
        "\n",
        "# apply one-hot encoding\n",
        "train_concat[\"y\"] = train_concat[\"y\"].apply(one_hot_encoding)\n",
        "\n",
        "# stack images and labels\n",
        "xtrain = np.stack(train_concat[\"X\"])\n",
        "ytrain = np.stack(train_concat[\"y\"])\n",
        "\n",
        "\n",
        "## VALIDATION SET\n",
        "val_concat = pd.concat([shuffled_mild_val, shuffled_moderate_val, shuffled_non_val, shuffled_verymild_val])\n",
        "val_concat = val_concat.sample(frac=1)\n",
        "val_concat[\"X\"] = val_concat[\"X\"].apply(load_img)\n",
        "val_concat[\"y\"] = val_concat[\"y\"].apply(one_hot_encoding)\n",
        "xval = np.stack(val_concat[\"X\"])\n",
        "yval = np.stack(val_concat[\"y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mh2aixZNslK"
      },
      "outputs": [],
      "source": [
        "## TEST SET\n",
        "test_concat = pd.concat([mild_test, moderate_test, non_test, verymild_test])\n",
        "test_concat = test_concat.sample(frac=1)\n",
        "test_concat[\"X\"] = test_concat[\"X\"].apply(load_img)\n",
        "test_concat[\"y\"] = test_concat[\"y\"].apply(one_hot_encoding)\n",
        "xtest = np.stack(val_concat[\"X\"])\n",
        "ytest = np.stack(val_concat[\"y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez1lk2J8J1jl"
      },
      "outputs": [],
      "source": [
        "# plot some images per class\n",
        "\n",
        "ncols = 10\n",
        "\n",
        "fig, axs = plt.subplots(nrows=4, ncols=ncols, figsize=(20, 10))\n",
        "\n",
        "for fname,ax in zip(shuffled_non.loc[:ncols, \"X\"], axs[0,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "\n",
        "for fname,ax in zip(shuffled_verymild.loc[:ncols, \"X\"], axs[1,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    \n",
        "for fname,ax in zip(shuffled_mild.loc[:ncols, \"X\"], axs[2,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "\n",
        "for fname,ax in zip(shuffled_moderate.loc[:ncols, \"X\"], axs[3,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "\n",
        "axs[0,0].set_ylabel(\"Non Dementia\")\n",
        "axs[1,0].set_ylabel(\"Very Mild Dementia\")\n",
        "axs[2,0].set_ylabel(\"Mild Dementia\")\n",
        "axs[3,0].set_ylabel(\"Moderate Dementia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJsdJAPsm6YZ"
      },
      "outputs": [],
      "source": [
        "# resnet50\n",
        "base_model = ResNet50(input_shape=(224,224,3), \n",
        "                   include_top=False,\n",
        "                   weights=\"imagenet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-NJ3UD_J6a7"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu5uYdRdOaAZ"
      },
      "outputs": [],
      "source": [
        "data = (xtrain, ytrain, xtest, ytest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFZ36aVBmyKC"
      },
      "outputs": [],
      "source": [
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"alzheimer_model.h5\",\n",
        "                                                    save_best_only=True)\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                     restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.5, verbose=1, mode=\"auto\",\n",
        "                              cooldown=5, patience=10, min_lr=0.00001)\n",
        "\n",
        "# Adding class weights to compensate class imabalance\n",
        "# class_weights = { 0: 1,\n",
        "#                   1: 1.5,\n",
        "#                   2: 3,\n",
        "#                   3: 20\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDMkMxONnURF"
      },
      "outputs": [],
      "source": [
        "def build_model(image_shape, **kwargs):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(ResNet50(input_shape=(224,224,3), \n",
        "                   include_top=False,\n",
        "                   weights=\"imagenet\"))\n",
        "    \n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    for _ in range(kwargs['conv_layers']):\n",
        "        model.add(SeparableConv2D(kwargs['kernels'], kernel_size=(kwargs['kernel_size'],kwargs['kernel_size']), activation='relu', padding='same'))\n",
        "        model.add(SeparableConv2D(kwargs['kernels'], kernel_size=(kwargs['kernel_size'],kwargs['kernel_size']), activation='relu', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        if kwargs['maxpooling'] == 1:\n",
        "          model.add(MaxPooling2D(pool_size=(1)))\n",
        "        if kwargs['dropout_cnn']  == 1:\n",
        "          model.add(Dropout(kwargs['dropout_perc_cnn']))\n",
        "    \n",
        "    model.add(AveragePooling2D(pool_size=1))\n",
        "    model.add(Flatten())\n",
        "    \n",
        "    for _ in range(kwargs['layers']):\n",
        "        model.add(Dense(kwargs['neurons'], activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        if kwargs['dropout']  == 1:\n",
        "          model.add(Dropout(kwargs['dropout_perc_cnn']))\n",
        "    \n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCsL3l8jnejv"
      },
      "outputs": [],
      "source": [
        "def fit_params(image_shape, data, **kwargs):\n",
        "\n",
        "    for k in kwargs.keys():\n",
        "        if 'perc' in k:\n",
        "            continue\n",
        "        kwargs[k]=kwargs[k].astype(np.int64)\n",
        "      \n",
        "    model = build_model(image_shape, **kwargs)\n",
        "    \n",
        "    # categorical crossentropy for loss and Adam for optimiser\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    if not os.path.exists(f'models/{image_shape[0]}'):\n",
        "        os.makedirs(f'models/{image_shape[0]}')\n",
        "    \n",
        "    callbacks = [checkpoint_cb, early_stopping_cb, reduce_lr]\n",
        "    \n",
        "    model.fit(xtrain, ytrain, validation_data=(xval, yval), batch_size=kwargs['batch_size'], epochs=20, verbose=0, callbacks=callbacks)\n",
        "    \n",
        "    score = model.evaluate(xtest, ytest)\n",
        "\n",
        "    model.save(f'models/{image_shape[0]}/{score[1]}.hdf5')\n",
        "\n",
        "    return score[1]\n",
        "\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "fit_with_partial = partial(fit_params, IMAGE_SIZE, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fivlWOoCnm5M"
      },
      "outputs": [],
      "source": [
        "image_shape=(224, 224, 3)\n",
        "\n",
        "\n",
        "fit_with_partial = partial(fit_params, image_shape, data)\n",
        "\n",
        "# Boundary definitions\n",
        "pbounds = {'layers':(1,9), 'neurons':(50, 1000), 'batch_size':(16, 64), 'dropout': (0,1), 'dropout_perc': (0.1,0.5), \n",
        "          'dropout_perc_cnn':(0.3, 1.0), 'dropout_cnn':(0,1), 'maxpooling': (0,1), 'conv_layers':(1,6), \n",
        "           'kernel_size':(1, 5), 'kernels':(32, 2048)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=fit_with_partial,\n",
        "    pbounds=pbounds,\n",
        "    verbose=2, \n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "optimizer.maximize()\n",
        "\n",
        "for i, res in enumerate(optimizer.res):\n",
        "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
        "\n",
        "print(optimizer.max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fKTCUnlnqnF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "bayesian_optimization-alzheimer.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}