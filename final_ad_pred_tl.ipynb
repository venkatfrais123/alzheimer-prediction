{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p58H4hmq5oQN"
      },
      "outputs": [],
      "source": [
        "print('Alzheimer Detection - 8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMLViWVZ1IBM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3sMlBmg51Wr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "import os \n",
        "from os.path import join\n",
        "import tqdm\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "from skimage.color import grey2rgb\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, AveragePooling2D, MaxPooling2D, Conv2D, SeparableConv2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical,plot_model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, \\\n",
        "    confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "import PIL\n",
        "import random\n",
        "import cv2 as cv\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7G-YO7S6CUV"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "    \n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-CE53uI6Nt2"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE # controls optimization of computational power\n",
        "BATCH_SIZE = 32 * strategy.num_replicas_in_sync # subset of the data to calculate gradient\n",
        "IMAGE_SIZE = [224, 224] # input image size for the first layer of the model\n",
        "EPOCHS = 100 # number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRIEsRLF6Yfq"
      },
      "outputs": [],
      "source": [
        "data_folder = \"/content/drive/MyDrive/MS-Upgrad/alzheimers-1\" # including both synthetic and real data\n",
        "\n",
        "## images NonDemented\n",
        "df_NonDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"NonDemented\", \"*\"))),\n",
        "    \"y\": 0,\n",
        "    \"class\": \"NonDementia\"\n",
        "})\n",
        "shuffled_non = df_NonDemented_imgs.sample(frac=1)\n",
        "testsize_non = int(0.2 * len(shuffled_non))\n",
        "# Test\n",
        "non_test = shuffled_non[:testsize_non]\n",
        "# Train and validation\n",
        "non_trainval = shuffled_non[testsize_non:]\n",
        "trainsize_non = int(0.8 * len(non_trainval))\n",
        "# Train\n",
        "shuffled_non_train = non_trainval[:trainsize_non]\n",
        "#Validation\n",
        "shuffled_non_val = non_trainval[trainsize_non:]\n",
        "\n",
        "\n",
        "## images VeryMildDemented\n",
        "df_VeryMildDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"VeryMildDemented\", \"*\"))),\n",
        "    \"y\": 1,\n",
        "    \"class\": \"VeryMildDementia\"\n",
        "})\n",
        "shuffled_verymild = df_VeryMildDemented_imgs.sample(frac=1)\n",
        "testsize_verymild = int(0.2 * len(shuffled_verymild))\n",
        "\n",
        "# Test\n",
        "verymild_test = shuffled_verymild[:testsize_verymild]\n",
        "\n",
        "# Train and validation\n",
        "verymild_trainval = shuffled_verymild[testsize_verymild:]\n",
        "trainsize_verymild = int(0.8 * len(verymild_trainval))\n",
        "\n",
        "# Train\n",
        "shuffled_verymild_train = verymild_trainval[:trainsize_verymild]\n",
        "\n",
        "#Validation\n",
        "shuffled_verymild_val = verymild_trainval[trainsize_verymild:]\n",
        "\n",
        "\n",
        "## images MildDemented\n",
        "df_MildDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"MildDemented\", \"*\"))),\n",
        "    \"y\": 2,\n",
        "    \"class\": \"MildDementia\"\n",
        "})\n",
        "shuffled_mild = df_MildDemented_imgs.sample(frac=1)\n",
        "testsize_mild = int(0.2 * len(shuffled_mild))\n",
        "\n",
        "# Test\n",
        "mild_test = shuffled_mild[:testsize_mild]\n",
        "\n",
        "# Train and validation\n",
        "mild_trainval = shuffled_mild[testsize_mild:]\n",
        "trainsize_mild = int(0.8 * len(mild_trainval))\n",
        "\n",
        "# Train\n",
        "shuffled_mild_train = mild_trainval[:trainsize_mild]\n",
        "\n",
        "#Validation\n",
        "shuffled_mild_val = mild_trainval[trainsize_mild:]\n",
        "\n",
        "## images ModerateDemented\n",
        "df_ModerateDemented_imgs = pd.DataFrame({\n",
        "    \"X\": sorted(glob(join(data_folder, \"ModerateDemented\", \"*\"))),\n",
        "    \"y\": 3,\n",
        "    \"class\": \"ModerateDementia\"\n",
        "})\n",
        "shuffled_moderate = df_ModerateDemented_imgs.sample(frac=1)\n",
        "testsize_moderate = int(0.2 * len(shuffled_moderate))\n",
        "\n",
        "# Test\n",
        "moderate_test = shuffled_moderate[:testsize_moderate]\n",
        "\n",
        "# Train and validation\n",
        "moderate_trainval = shuffled_moderate[testsize_moderate:]\n",
        "trainsize_moderate = int(0.8 * len(moderate_trainval))\n",
        "\n",
        "# Train\n",
        "shuffled_moderate_train = moderate_trainval[:trainsize_moderate]\n",
        "\n",
        "#Validation\n",
        "shuffled_moderate_val = moderate_trainval[trainsize_moderate:]\n",
        "\n",
        "\n",
        "## Number of images\n",
        "print(\"TOTAL:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(shuffled_non))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(shuffled_verymild))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(shuffled_mild))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(shuffled_moderate))\n",
        "print(\"------------\")\n",
        "print(\"\\nTest:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(non_test))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(verymild_test))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(mild_test))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(moderate_test))\n",
        "print(\"------------\")\n",
        "print(\"\\nTraining:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(shuffled_non_train))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(shuffled_verymild_train))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(shuffled_mild_train))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(shuffled_moderate_train))\n",
        "print(\"------------\")\n",
        "print(\"\\nValidation:\")\n",
        "print(\"# of images with NonDemented Alzheimer =\", len(shuffled_non_val))\n",
        "print(\"# of images with VeryMildDemented Alzheimer =\", len(shuffled_verymild_val))\n",
        "print(\"# of images with MildDemented Alzheimer =\", len(shuffled_mild_val))\n",
        "print(\"# of images with ModerateDemented Alzheimer =\", len(shuffled_moderate_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoRCc_1cVXHA"
      },
      "outputs": [],
      "source": [
        "# class imbalance in the dataset\n",
        "\n",
        "heights = [len(shuffled_non), len(shuffled_verymild), len(shuffled_mild), len(shuffled_moderate)]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "height = heights\n",
        "bars = ('Non', 'Very Mild', 'Mild', 'Moderate')\n",
        "y_pos = np.arange(len(bars))\n",
        "plt.bar(y_pos, height)\n",
        "plt.xticks(y_pos, bars)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mVuJLev6m44"
      },
      "outputs": [],
      "source": [
        "def load_img(fname):\n",
        "    img = cv.imread(fname)\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "    resize = (IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
        "    img = cv.resize(img, resize)\n",
        "    return img\n",
        "\n",
        "# one_hot_encoding\n",
        "def one_hot_encoding(class_number):\n",
        "    ha = [0,0,0,0]\n",
        "    ha[class_number] = 1\n",
        "    return ha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRqgNO2GsXpf"
      },
      "outputs": [],
      "source": [
        "## TRAIN SET\n",
        "# concatenate\n",
        "train_concat = pd.concat([shuffled_mild_train, shuffled_moderate_train, shuffled_non_train, shuffled_verymild_train])\n",
        "# shuffle\n",
        "train_concat = train_concat.sample(frac=1)\n",
        "# load image\n",
        "train_concat[\"X\"] = train_concat[\"X\"].apply(load_img)\n",
        "\n",
        "# extract label before one hot in order to use for class weights\n",
        "y_train_lab = train_concat[\"y\"]\n",
        "y_train_lab = np.array(y_train_lab) # convert into a numpy array\n",
        "\n",
        "# apply one-hot encoding\n",
        "train_concat[\"y\"] = train_concat[\"y\"].apply(one_hot_encoding)\n",
        "\n",
        "# stack images and labels\n",
        "xtrain = np.stack(train_concat[\"X\"])\n",
        "ytrain = np.stack(train_concat[\"y\"])\n",
        "\n",
        "\n",
        "## VALIDATION SET\n",
        "val_concat = pd.concat([shuffled_mild_val, shuffled_moderate_val, shuffled_non_val, shuffled_verymild_val])\n",
        "val_concat = val_concat.sample(frac=1)\n",
        "val_concat[\"X\"] = val_concat[\"X\"].apply(load_img)\n",
        "val_concat[\"y\"] = val_concat[\"y\"].apply(one_hot_encoding)\n",
        "xval = np.stack(val_concat[\"X\"])\n",
        "yval = np.stack(val_concat[\"y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ieAE-Fsd2z"
      },
      "outputs": [],
      "source": [
        "# plot some images per class\n",
        "\n",
        "ncols = 10\n",
        "\n",
        "fig, axs = plt.subplots(nrows=4, ncols=ncols, figsize=(20, 10))\n",
        "\n",
        "for fname,ax in zip(shuffled_non.loc[:ncols, \"X\"], axs[0,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "\n",
        "for fname,ax in zip(shuffled_verymild.loc[:ncols, \"X\"], axs[1,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    \n",
        "for fname,ax in zip(shuffled_mild.loc[:ncols, \"X\"], axs[2,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "\n",
        "for fname,ax in zip(shuffled_moderate.loc[:ncols, \"X\"], axs[3,:]):\n",
        "    im = load_img(fname)\n",
        "    ax.imshow(im)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "\n",
        "axs[0,0].set_ylabel(\"Non Dementia\")\n",
        "axs[1,0].set_ylabel(\"Very Mild Dementia\")\n",
        "axs[2,0].set_ylabel(\"Mild Dementia\")\n",
        "axs[3,0].set_ylabel(\"Moderate Dementia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHAYZn6U5J2N"
      },
      "outputs": [],
      "source": [
        "# Transfer Learning Model\n",
        "base_model = ResNet50(input_shape=(224,224,3), \n",
        "                   include_top=False,\n",
        "                   weights=\"imagenet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kAzU8GI5PkW"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g24-uMKG0Esc"
      },
      "outputs": [],
      "source": [
        "# Convolutional Block\n",
        "def conv_block(filters):\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='elu', padding='same'),\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='elu', padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPool2D()\n",
        "    ]\n",
        "    )\n",
        "    \n",
        "    return block\n",
        "\n",
        "# Dense Block\n",
        "def dense_block(units, dropout_rate):\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units, activation='elu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    \n",
        "    return block\n",
        "\n",
        "# Function to build the model\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        # tf.keras.Input(shape=(*IMAGE_SIZE, 3)),\n",
        "        tf.keras.applications.resnet50.ResNet50(\n",
        "             include_top=False,\n",
        "             weights='imagenet',\n",
        "             input_shape=(224,224,3),\n",
        "        ),\n",
        "\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        \n",
        "        conv_block(32),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=1),\n",
        "\n",
        "        conv_block(64),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=1),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        conv_block(128),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=1),\n",
        "        \n",
        "        conv_block(256),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=1),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        conv_block(512),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=1),\n",
        "\n",
        "        tf.keras.layers.AveragePooling2D(pool_size=1),\n",
        "        \n",
        "        tf.keras.layers.Flatten(),\n",
        "        dense_block(512, 0.7),\n",
        "        dense_block(128, 0.5),\n",
        "        dense_block(64, 0.3),\n",
        "        \n",
        "        tf.keras.layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JG95n-a0WgX"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = build_model()\n",
        "\n",
        "    METRICS = [tf.keras.metrics.AUC(name='auc')]\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=tf.losses.CategoricalCrossentropy(),\n",
        "        metrics=METRICS\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "TUqCKYEo-T1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1rGTqhF0ZKz"
      },
      "outputs": [],
      "source": [
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"alzheimer_model.h5\",\n",
        "                                                    save_best_only=True)\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                     restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='auc', factor=0.5, verbose=1, mode=\"auto\",\n",
        "                              cooldown=5, patience=10, min_lr=0.00001)\n",
        "\n",
        "# Adding class weights to compensate class imabalance\n",
        "# class_weights = { 0: 1,\n",
        "#                   1: 1.5,\n",
        "#                   2: 3,\n",
        "#                   3: 20\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTwSSwa60pcV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fit the model\n",
        "# history = model.fit(xtrain, ytrain, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[checkpoint_cb, early_stopping_cb, reduce_lr], validation_data=(xval, yval), class_weight = class_weights)\n",
        "history = model.fit(xtrain, ytrain, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[checkpoint_cb, early_stopping_cb, reduce_lr], validation_data=(xval, yval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4xnxUCm0qS5"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(3, 1, figsize=(20, 20))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['auc', 'loss']):\n",
        "    ax[i].plot(history.history[met])\n",
        "    ax[i].plot(history.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "ax[2].plot(history.history['lr'])\n",
        "ax[2].set_title('Model lr')\n",
        "ax[2].set_xlabel('epochs')\n",
        "ax[2].set_ylabel('lr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vPGMHYQ4VmF"
      },
      "outputs": [],
      "source": [
        "## TEST SET\n",
        "test_concat = pd.concat([mild_test, moderate_test, non_test, verymild_test])\n",
        "test_concat = test_concat.sample(frac=1)\n",
        "test_concat[\"X\"] = test_concat[\"X\"].apply(load_img)\n",
        "test_concat[\"y\"] = test_concat[\"y\"].apply(hot_array)\n",
        "xtest = np.stack(val_concat[\"X\"])\n",
        "ytest = np.stack(val_concat[\"y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka9FZts84Zgr"
      },
      "outputs": [],
      "source": [
        "# evaluation\n",
        "_ = model.evaluate(xtest,ytest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofc1-Q0J4a6N"
      },
      "outputs": [],
      "source": [
        "# Federico Agostini notebook Snippet for metrics\n",
        "\n",
        "def print_metrics(model, X, y, cm_labels=\"auto\"):\n",
        "    \"\"\"\n",
        "    Print the following metrics: accuracy, balanced accuracy, precision, recall, f1.\n",
        "    If the model is able to predict probabilities, also auc is calculated.\n",
        "    Moreover, the confusion matrix is plotted.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : sklearn estimator\n",
        "        Sklearn estimator or similar which implements the method `predict`\n",
        "        and optionally `predict_proba`.\n",
        "    X : array like\n",
        "        Input features.\n",
        "    y : array like\n",
        "        Target labels.\n",
        "    cm_labels : list [default=\"auto\"]\n",
        "        Optional labels to be used in the confusion matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict\n",
        "        Dictionary with the calculated metrics.\n",
        "\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.squeeze(y_pred)\n",
        "    y_pred = np.argmax(y_pred,1).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\"    : accuracy_score(y, y_pred),\n",
        "        \"Bal Accuracy\": balanced_accuracy_score(y, y_pred),\n",
        "        \"Precision\"   : precision_score(y, y_pred, average=\"macro\"),\n",
        "        \"Recall\"      : recall_score(y, y_pred, average=\"macro\"),\n",
        "        \"f1\"          : f1_score(y, y_pred, average=\"macro\")\n",
        "    }\n",
        "    # add AUC if the classifier is able to predict probabilities\n",
        "    try:\n",
        "        y_pred_proba = model.predict(X)\n",
        "        metrics[\"AUC\"] = roc_auc_score(y, y_pred_proba, multi_class=\"ovr\", average=\"macro\")\n",
        "    except:\n",
        "        metrics[\"AUC\"] = np.nan\n",
        "    \n",
        "    for k,v in metrics.items():\n",
        "        print(\"{:12s} = {}\".format(k,v))  \n",
        "    print(\"\\n\")\n",
        "    \n",
        "    # confusion matrix\n",
        "    cm = confusion_matrix(y, y_pred, normalize=\"true\")\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    sns.heatmap(cm, ax=ax, square=True, vmin=0, vmax=1, annot=True, \n",
        "                linewidths=.05, fmt=\".2f\", cbar_kws={\"shrink\":.8}, \n",
        "                xticklabels=cm_labels, yticklabels=cm_labels)\n",
        "    plt.xticks([0.5, 1.5, 2.5, 3.5], ['Non', 'Very Mild', 'Mild', 'Moderate'])\n",
        "    plt.yticks([0.5, 1.5, 2.5, 3.5], ['Non', 'Very Mild', 'Mild', 'Moderate'])\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "\n",
        "    metrics[\"cm\"] = cm\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUvEvB7B4fra"
      },
      "outputs": [],
      "source": [
        "# visualize model metrics\n",
        "y_test = ytest.argmax(axis=1)\n",
        "met = print_metrics(model, xtest, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcylCzPxylid"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "final-ad-pred-tl.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}